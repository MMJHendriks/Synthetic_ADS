---
title: "Synthesis"
author: "Stijn van den Broek"
date: "02/05/2021"
output: html_document
---

```{r}
library(ggplot2)
library(tidyverse)
library(magrittr)
library(mice)
library(furrr)
library(purrr)
library(broom)
library(caret)
library(mboost)
```

## Reading the data

```{r}
data = read.csv('diabetes.csv')
data$Outcome = factor(data$Outcome, levels = c(0, 1))
```

## Setting the seed

```{r}
set.seed(123)
```

## Synthesizing function and imputation methods

```{r}
cart = rep("cart", ncol(data))
names(cart) = colnames(data)

custom_method = c("cart", "pmm", "pmm", "pmm", "cart", "pmm", "cart", "cart", "logreg")
names(custom_method) = colnames(data)


synthesize = function(dataframe, method = custom_method, m = 5, nsim = 100) {
  method = method
  predict <- make.predictorMatrix(dataframe)
  syn = future_map(1:nsim, ~ { 
  dataframe %>% mice(m=m,
                    method = method,
                    predictorMatrix = predict,
                    where=matrix(TRUE, nrow(dataframe), ncol(dataframe)),
                    print=FALSE)
  }, .options=future_options(seed=as.integer(123)), .progress=T, .id = "syn")
  
  return(syn)
}
```

## Pooling function

```{r}
pool3.syn = function(mira) {
  
  if(class(mira)[1] == "mira") { # if the input object is of class mira
    fitlist <- mira %$% analyses # extract the analyses from the mira object
  }
  else {
    fitlist <- mira              # and otherwise, just take the input list
  }
  
  m <- length(fitlist)           # number of imputations
  
  pooled <- fitlist %>% 
    map_dfr(broom::tidy) %>%
    group_by(term) %>%
    summarise(est     = mean(estimate),
              bm      = sum((estimate - est)^2) / (m - 1),
              ubar    = mean(std.error^2),
              var     = ubar + bm/m, # new variance estimate
              df      = (m - 1) * (1 + (ubar * m)/bm), # and new df estimate
              lower   = est - qt(.975, df) * sqrt(var),
              upper   = est + qt(.975, df) * sqrt(var), .groups = 'drop')
  
  return(pooled)
}
```

## Confidence interval function

```{r}
ci_cov = function(pooled, true_fit = NULL, coefs = NULL, vars = NULL) {
  
  if (!is.null(true_fit)) {
    coefs = coef(true_fit)
    vars = diag(vcov(true_fit))
  }
  
  nsim = nrow(pooled) / length(coefs)
  
  pooled %>% mutate(true_coef = rep(coefs, nsim),
                    true_var  = rep(vars, nsim),
                    cover     = lower < true_coef & true_coef < upper) %>%
    group_by(term) %>%
    summarise("True Est" = unique(true_coef),
              "Syn Est"  = mean(est),
              "Bias"     = mean(est - true_coef),
              "True SE"  = unique(sqrt(true_var)),
              "Syn SE"   = mean(sqrt(var)),
              "df"       = mean(df),
              "Lower"    = mean(lower),
              "Upper"    = mean(upper),
              "CIW"      = mean(upper - lower),
              "Coverage" = mean(cover), .groups = "drop")
}
```

## Prediction function

```{r}
predict_synth_or_true = function(data, model) {
  set.seed(123) #for reproducibility
  predicted_values = predict(object = model, newdata = data)
  true_values = data[,ncol(data)]
  
  result = data.frame(true_value = factor(true_values, levels = c(0, 1)), predicted_value = factor(predicted_values, levels = c(0, 1)))

  return(result)
}
```

## 5a. Synthesizing the data

```{r}
synthesized = synthesize(data, nsim = 10)
```

## 5c. Running the analysis model on each of the synthetic sets

```{r}
synthetic_analysis = analysis_model(data = synthesized, print = FALSE, synthetic = TRUE) #function is found in Analysis_Stijn.rmd
```

## 5f. Run a prediction model that predicts syn or true
Eerst een model trainen op ~30% van de data, dan de rest predicten en kijken wat de accuracy is. Caret met method logreg?

```{r}
total_result = data.frame() #data frame in which the results of all sampled data will be stored
for (i in seq(1, length(synthesized), 1)) { #looping through each simulation
  imputation = synthesized[[i]]
  for (n in seq(1, imputation$m, 1)) { #looping through each imputation of the simulation

    syndata = complete(imputation, action = n)
    
    true_data = sample_n(data, (nrow(data)*0.5)) #sampling half of the original data
    true_data$Synth_or_true = 0 #setting the true data to 0
    
    synthetic_data = sample_n(syndata, (nrow(syndata)*0.5)) #sampling half of the synthetic data
    synthetic_data$Synth_or_true = 1 #setting the synthetic data to 1
    
    prediction_data = rbind(true_data, synthetic_data) #combining the two samples into a single data set
    prediction_data$Synth_or_true = factor(prediction_data$Synth_or_true, levels = c(0, 1))
    
    train_index = sample(seq_len(nrow(prediction_data)), size = floor(0.30 * nrow(prediction_data))) #creating a list of indices that will be the training data

    train = prediction_data[train_index,]
    test = prediction_data[-train_index,]
    
    prediction_model = train(Synth_or_true ~ ., data = train, method = "glmboost") #prediction model with method glmboost for classification

    result = predict_synth_or_true(test, prediction_model) #using the prediction function to get a data frame of true and predicted values
    total_result = rbind(total_result, result) #appending the data frame to the overarching data frame
  }
}

cfmatrix = confusionMatrix(total_result$predicted_value, total_result$true_value) #creating a confusion matrix
print(cfmatrix)
print(prediction_model)
```

## 5g. Pooling the model parameters over the sets and recording the accuracy

```{r}
pooled = map_dfr(synthesized, function(x) {
  x %$%
  glm(Outcome ~ Pregnancies + Glucose, family = 'binomial') %>%
  pool3.syn()
})
```

## 6e. Calculating the 95% coverage rates of the parameters

```{r}
true_model = glm(Outcome~Pregnancies+Glucose, data = data, family = "binomial")
confidence_interval = ci_cov(pooled, true_fit = true_model)
```
