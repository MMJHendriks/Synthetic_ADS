---
title: "Synthetic data"
author: "Stijn van den Broek"
date: "02/05/2021"
output: html_document
---

```{r}
library(ggplot2)
library(tidyverse)
library(magrittr)
library(corrplot)
library(mice)
library(furrr)
library(purrr)
library(broom)
library(caret)
library(mboost)
```

## Reading the data

```{r}
data = read.csv('diabetes.csv')
data$Outcome = factor(data$Outcome, levels = c(0, 1))
```

## Setting the seed

```{r}
set.seed(123)
```

## 1. formulating an analysis model

```{r}
analysis_model = function(data, print = FALSE, synthetic = FALSE) {

  analyse_dataframe = function(model) { #function for getting the coefficients of the data in a list
    coefficient_row = rep(NA, 0)
    for (i in seq(1, nrow(tidy(model)), 1)) {
      row = tidy(model)[i,]
      names(row) = paste0(row$term, '_', names(row))
      row = row[-1]
      coefficient_row = c(coefficient_row, row)
    }
   
    if (print == TRUE) {
      print(summary(model))
    }
    
    return(coefficient_row)
  }

  if (synthetic == TRUE) {
    coefficient_dataframe = data.frame()
    nsim = 1
    for (x in seq(1, length(data), 1)) {
      m = 1
      imputations = data[[x]]
      for (n in seq(1, imputations$m, 1)) {
        synth_data = complete(imputations, action = n)
        synthetic_model = glm(Outcome~Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age, data = synth_data, family = "binomial") #very clunky implementation, but it works for now.
        synthetic_coefficients = analyse_dataframe(synthetic_model)
        synthetic_coefficients = synthetic_coefficients %>% 
          c(nsim) %>% 
          c(m) %>% 
          data.frame()
        names(synthetic_coefficients)[(ncol(synthetic_coefficients)-1)] = "nsim"
        names(synthetic_coefficients)[ncol(synthetic_coefficients)] = "m"
        synthetic_coefficients = cbind(synthetic_coefficients, statistical_properties(synth_data, print = FALSE)) #adding the statistical properties of the data
        coefficient_dataframe = rbind(coefficient_dataframe, synthetic_coefficients)
        m = m+1
      }
      nsim = nsim+1
    }
  }
  
  else {
    model = glm(Outcome~Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age, data = data, family = "binomial") #also clunky.
    coefficient_dataframe = data.frame(analyse_dataframe(model))
    coefficient_dataframe = cbind(coefficient_dataframe, statistical_properties(data, print = FALSE)) #adding the statistical properties of the data
  }
  
  return(coefficient_dataframe)
}
```

## Function for obtaining statistical properties

```{r}
statistical_properties = function(dataframe, print = TRUE) {
  
  make_wide_columns = function(list, statistic) {
    statistic_columns = data.frame(placeholder = NA)
    for (i in seq(1, length(list), 1)) {
      column_name = paste0(names(list)[i], '_', statistic)
      value = list[i]
      single_column = data.frame(placeholder = value)
      colnames(single_column) = column_name
      statistic_columns = cbind(statistic_columns, single_column)
    }
    
    statistic_columns = statistic_columns[,-1]
    return(statistic_columns)
  }
  
  mean = lapply(dataframe, mean, na.rm = TRUE)
  variance = apply(dataframe, 2, var)
  standard_deviation = apply(dataframe, 2, sd)
  
  if (print == TRUE) {
    print(corrplot(cor(dataframe), method = "color"))
  }
  
  meancols = make_wide_columns(mean, "mean")
  varcols = make_wide_columns(variance, "var")
  sdcols = make_wide_columns(standard_deviation, "sd")
  
  statistics = cbind(meancols, varcols, sdcols)
  
  return(statistics)
}
```

## Function for plotting distributions

```{r}
dataset_distribution = function(dataframe, bins = 20) {
  ggplot(gather(dataframe), aes(value)) + 
    geom_histogram(bins = bins) + 
    facet_wrap(~key, scales = 'free_x')
}
```

## 2. Running the analysis model on the data set to obtain the true data inference

```{r}
analysis = analysis_model(data = data, print = FALSE, synthetic = FALSE)
```

## 3. Obtaining the statistical properties of the true data set

```{r}
true_statistics = statistical_properties(data)
```

## 4. Plotting the distributions of the true data set

```{r}
dataset_distribution(data)
```

## Synthesizing function and imputation methods

```{r}
cart = rep("cart", ncol(data))
names(cart) = colnames(data)

custom_method = c("cart", "pmm", "pmm", "pmm", "cart", "pmm", "cart", "cart", "logreg")
names(custom_method) = colnames(data)


synthesize = function(dataframe, method = custom_method, m = 5, nsim = 100) {
  method = method
  predict <- make.predictorMatrix(dataframe)
  syn = future_map(1:nsim, ~ { 
  dataframe %>% mice(m=m,
                    method = method,
                    predictorMatrix = predict,
                    where=matrix(TRUE, nrow(dataframe), ncol(dataframe)),
                    print=FALSE)
  }, .options=future_options(seed=as.integer(123)), .progress=T, .id = "syn")
  
  return(syn)
}
```

## Pooling function

```{r}
pool3.syn = function(mira) {
  
  if(class(mira)[1] == "mira") { # if the input object is of class mira
    fitlist <- mira %$% analyses # extract the analyses from the mira object
  }
  else {
    fitlist <- mira              # and otherwise, just take the input list
  }
  
  m <- length(fitlist)           # number of imputations
  
  pooled <- fitlist %>% 
    map_dfr(broom::tidy) %>%
    group_by(term) %>%
    summarise(est     = mean(estimate),
              bm      = sum((estimate - est)^2) / (m - 1),
              ubar    = mean(std.error^2),
              var     = ubar + bm/m, # new variance estimate
              df      = (m - 1) * (1 + (ubar * m)/bm), # and new df estimate
              lower   = est - qt(.975, df) * sqrt(var),
              upper   = est + qt(.975, df) * sqrt(var), .groups = 'drop')
  
  return(pooled)
}
```

## Confidence interval function

```{r}
ci_cov = function(pooled, true_fit = NULL, coefs = NULL, vars = NULL) {
  
  if (!is.null(true_fit)) {
    coefs = coef(true_fit)
    vars = diag(vcov(true_fit))
  }
  
  nsim = nrow(pooled) / length(coefs)
  
  pooled %>% mutate(true_coef = rep(coefs, nsim),
                    true_var  = rep(vars, nsim),
                    cover     = lower < true_coef & true_coef < upper) %>%
    group_by(term) %>%
    summarise("True Est" = unique(true_coef),
              "Syn Est"  = mean(est),
              "Bias"     = mean(est - true_coef),
              "True SE"  = unique(sqrt(true_var)),
              "Syn SE"   = mean(sqrt(var)),
              "df"       = mean(df),
              "Lower"    = mean(lower),
              "Upper"    = mean(upper),
              "CIW"      = mean(upper - lower),
              "Coverage" = mean(cover), .groups = "drop")
}
```

## Prediction function

```{r}
predict_synth_or_true = function(data, model) {
  set.seed(123) #for reproducibility
  predicted_values = predict(object = model, newdata = data)
  true_values = data[,ncol(data)] #it just takes the values in the last column of the data frame so this is very prone to errors
  result = data.frame(true_value = factor(true_values, levels = c(0, 1)), predicted_value = factor(predicted_values, levels = c(0, 1)))

  return(result)
}
```

## 5a. Synthesizing the data

```{r}
synthesized = synthesize(data, nsim = 10)
```

## 5b, 5c & 5d. Calculating the statistical properties, running the analysis model on each of the synthetic sets & combining the statistics and parameters into single inferences

```{r}
synthetic_analysis = analysis_model(data = synthesized, print = FALSE, synthetic = TRUE)
```

## 5e & 5f. Combining half of the imputed set randomly with half of the true set for each of the m imputed sets & running a prediction model that predicts syn or true

```{r}
total_result = data.frame() #data frame in which the results of all sampled data will be stored
for (i in seq(1, length(synthesized), 1)) { #looping through each simulation
  imputation = synthesized[[i]]
  for (n in seq(1, imputation$m, 1)) { #looping through each imputation of the simulation

    syndata = complete(imputation, action = n)
    
    true_data = sample_n(data, (nrow(data)*0.5)) #sampling half of the original data
    true_data$Synth_or_true = 0 #setting the true data to 0
    
    synthetic_data = sample_n(syndata, (nrow(syndata)*0.5)) #sampling half of the synthetic data
    synthetic_data$Synth_or_true = 1 #setting the synthetic data to 1
    
    prediction_data = rbind(true_data, synthetic_data) #combining the two samples into a single data set
    prediction_data$Synth_or_true = factor(prediction_data$Synth_or_true, levels = c(0, 1))
    
    train_index = sample(seq_len(nrow(prediction_data)), size = floor(0.30 * nrow(prediction_data))) #creating a list of indices that will be the training data

    train = prediction_data[train_index,]
    test = prediction_data[-train_index,]
    
    prediction_model = train(Synth_or_true ~ ., data = train, method = "glm") #prediction model with method glm for classification

    result = predict_synth_or_true(test, prediction_model) #using the prediction function to get a data frame of true and predicted values
    total_result = rbind(total_result, result) #appending the data frame to the overarching data frame
  }
}

cfmatrix = confusionMatrix(total_result$predicted_value, total_result$true_value) #creating a confusion matrix
print(cfmatrix)
print(prediction_model)
```

## 5g. Pooling the model parameters over the sets and recording the accuracy

```{r}
pooled = map_dfr(synthesized, function(x) {
  x %$%
  glm(Outcome ~ Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age, family = 'binomial') %>%
  pool3.syn()
})

true_model = glm(Outcome~Pregnancies+Glucose+BloodPressure+SkinThickness+Insulin+BMI+DiabetesPedigreeFunction+Age, data = data, family = "binomial")
confidence_interval = ci_cov(pooled, true_fit = true_model)
```
